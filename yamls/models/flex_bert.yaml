model:
  name: flex_bert
  model_config:
    activation_function: silu
    attention_layer: base
    attention_probs_dropout_prob: 0.0
    attn_out_bias: False
    attn_out_dropout_prob: 0.0
    attn_qkv_bias: False
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: True
    embedding_layer: absolute_pos
    encoder_layer: base
    loss_function: cross_entropy
    loss_kwargs:
      reduction: mean
    mlp_dropout_prob: 0.0
    mlp_in_bias: False
    mlp_layer: mlp
    mlp_out_bias: False
    norm_kwargs:
      eps: 1e-6
    normalization: rmsnorm
    padding: unpadded