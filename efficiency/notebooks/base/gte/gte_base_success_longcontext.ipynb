{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import random\n",
    "import numpy as np\n",
    "def create_fixed_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed short context - all sequences 512 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 512), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 512, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_fixed_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed long context - all sequences 8192 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 8192), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 8192, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_variable_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable short context - normal dist around 256 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=256, std=64, size=(num_samples,)).int().clamp(32, 512)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((512,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(512, dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "def create_variable_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable long context - normal dist around 4096 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=4096, std=1024, size=(num_samples,)).int().clamp(32, 8192)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((int(8192),), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(int(8192), dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "# Create all datasets\n",
    "def create_all_datasets(tokenizer, num_samples=8192):\n",
    "    datasets = {\n",
    "        'fixed_short': create_fixed_short_dataset(tokenizer, num_samples),\n",
    "        'fixed_long': create_fixed_long_dataset(tokenizer, num_samples), \n",
    "        'variable_short': create_variable_short_dataset(tokenizer, num_samples),\n",
    "        'variable_long': create_variable_long_dataset(tokenizer, num_samples)\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Override attn_implementation='sdpa' to 'eager' as use_memory_efficient_attention=True\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModel.from_pretrained(\"Alibaba-NLP/gte-en-mlm-base\", trust_remote_code=True, unpad_inputs=True, use_memory_efficient_attention=True).to('cuda:0')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-en-mlm-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gte_datasets = create_all_datasets(tokenizer, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 36\n",
    "fixed_long_batch = {\n",
    "    'input_ids': gte_datasets['fixed_long']['input_ids'][:batch_size].to('cuda:0'),\n",
    "    'attention_mask': gte_datasets['fixed_long']['attention_mask'][:batch_size].to('cuda:0')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model(**fixed_long_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
