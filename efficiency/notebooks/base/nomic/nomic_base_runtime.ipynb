{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import random\n",
    "import numpy as np\n",
    "def create_fixed_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed short context - all sequences 512 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 512), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 512, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_fixed_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed long context - all sequences 8192 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 8192), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 8192, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_variable_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable short context - normal dist around 256 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=256, std=64, size=(num_samples,)).int().clamp(32, 512)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((512,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(512, dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "def create_variable_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable long context - normal dist around 4096 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=4096, std=1024, size=(num_samples,)).int().clamp(32, 8192)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((int(8192),), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(int(8192), dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "# Create all datasets\n",
    "def create_all_datasets(tokenizer, num_samples=8192):\n",
    "    datasets = {\n",
    "        'fixed_short': create_fixed_short_dataset(tokenizer, num_samples),\n",
    "        'variable_short': create_variable_short_dataset(tokenizer, num_samples),\n",
    "        'fixed_long': create_fixed_long_dataset(tokenizer, num_samples), \n",
    "        'variable_long': create_variable_long_dataset(tokenizer, num_samples)\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bc/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/eb02ceb48c1fdcc477ff1925c9732c379f0f0d1f/modeling_hf_nomic_bert.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "model = transformers.AutoModel.from_pretrained(\"nomic-ai/nomic-bert-2048\", trust_remote_code=True).to('cuda:0')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"nomic-ai/nomic-bert-2048\")\n",
    "datasets = create_all_datasets(tokenizer, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing fixed_short...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed_short -> 35.83 ± 0.00 sec (batch_size: 512)\n",
      "\n",
      "Testing variable_short...\n",
      "variable_short -> 35.83 ± 0.01 sec (batch_size: 512)\n",
      "\n",
      "Testing fixed_long...\n",
      "fixed_long -> 1458.79 ± 0.52 sec (batch_size: 32)\n",
      "\n",
      "Testing variable_long...\n",
      "variable_long -> 1455.46 ± 0.31 sec (batch_size: 32)\n",
      "\n",
      "Processing Time Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "bert-base-uncased Model:\n",
      "fixed_short: 35.83 ± 0.00 seconds (batch_size: 32)\n",
      "variable_short: 35.83 ± 0.01 seconds (batch_size: 32)\n",
      "fixed_long: 1458.79 ± 0.52 seconds (batch_size: 32)\n",
      "variable_long: 1455.46 ± 0.31 seconds (batch_size: 32)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Speed tests all ran at standardised batch sizes for the notebook -- impact on speed at 512 is not significant.\n",
    "batch_size = 512\n",
    "n_iters = 10\n",
    "times = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        batch_size = 512\n",
    "        print(f\"\\nTesting {dataset_name}...\")\n",
    "        batch_times = []\n",
    "\n",
    "        if 'long' in dataset_name:\n",
    "            batch_size = 32\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                dataset['input_ids'].to('cuda:0'),\n",
    "                dataset['attention_mask'].to('cuda:0')\n",
    "            ),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        for batch in dataloader:\n",
    "            model(input_ids=batch[0], attention_mask=batch[1])\n",
    "            break\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Timing runs\n",
    "        for i in range(n_iters):\n",
    "            start = time.perf_counter()\n",
    "            for batch in dataloader:\n",
    "                model(input_ids=batch[0], attention_mask=batch[1])\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            batch_times.append(end - start)\n",
    "            \n",
    "        mean_time = np.mean(batch_times)\n",
    "        std_time = np.std(batch_times)\n",
    "        print(f\"{dataset_name} -> {mean_time:.2f} ± {std_time:.2f} sec (batch_size: {batch_size})\")\n",
    "        times.append((dataset_name, mean_time, std_time))\n",
    "\n",
    "    print(\"\\nProcessing Time Summary:\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "    print(\"bert-base-uncased Model:\")\n",
    "    for name, mean, std in times:\n",
    "        print(f\"{name}: {mean:.2f} ± {std:.2f} seconds (batch_size: {batch_size})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save timing results to file\n",
    "with open('runtime.txt', 'w') as f:\n",
    "    for name, mean, std in times:\n",
    "        f.write(f\"{name}: {mean:.2f} ± {std:.2f} seconds (batch_size: {batch_size})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
