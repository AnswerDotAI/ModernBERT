{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import random\n",
    "import numpy as np\n",
    "def create_fixed_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed short context - all sequences 512 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 512), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 512, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_fixed_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Fixed long context - all sequences 8192 tokens\n",
    "    tokens = torch.randint(100, 16000, (num_samples, 8192), dtype=torch.long)\n",
    "    mask = torch.ones(num_samples, 8192, dtype=torch.long)\n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': mask.long()\n",
    "    }\n",
    "\n",
    "def create_variable_short_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable short context - normal dist around 256 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=256, std=64, size=(num_samples,)).int().clamp(32, 512)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((512,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(512, dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "def create_variable_long_dataset(tokenizer, num_samples=8192):\n",
    "    # Variable long context - normal dist around 4096 tokens\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    lengths = torch.normal(mean=4096, std=1024, size=(num_samples,)).int().clamp(32, 8192)\n",
    "    tokens_list = []\n",
    "    masks_list = []\n",
    "    for length in lengths:\n",
    "        # Generate random tokens of the specified length\n",
    "        tokens = torch.randint(100, 16000, (length.item(),))\n",
    "        # Create attention mask of 1s for the actual tokens\n",
    "        mask = torch.ones(length.item())\n",
    "        # Pad both tokens and mask to max length\n",
    "        padded_tokens = torch.full((int(8192),), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        padded_mask = torch.zeros(int(8192), dtype=torch.long)\n",
    "        padded_tokens[:length] = tokens\n",
    "        padded_mask[:length] = mask\n",
    "        tokens_list.append(padded_tokens)\n",
    "        masks_list.append(padded_mask)\n",
    "    \n",
    "    tokens = torch.stack(tokens_list)\n",
    "    masks = torch.stack(masks_list)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokens.long(),\n",
    "        'attention_mask': masks.long()\n",
    "    }\n",
    "\n",
    "# Create all datasets\n",
    "def create_all_datasets(tokenizer, num_samples=8192):\n",
    "    datasets = {\n",
    "        'fixed_short': create_fixed_short_dataset(tokenizer, num_samples),\n",
    "        'fixed_long': create_fixed_long_dataset(tokenizer, num_samples), \n",
    "        'variable_short': create_variable_short_dataset(tokenizer, num_samples),\n",
    "        'variable_long': create_variable_long_dataset(tokenizer, num_samples)\n",
    "    }\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bc/miniforge3/envs/bert24/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/bc/miniforge3/envs/bert24/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "model = transformers.AutoModel.from_pretrained(\"microsoft/deberta-v3-large\", trust_remote_code=True).to('cuda:0')\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "datasets = create_all_datasets(tokenizer, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing fixed_short...\n",
      "fixed_short -> 170.79 ± 0.06 sec (batch_size: 128)\n",
      "Skipping long datasets for short encoders...\n",
      "Skipping variable datasets for no-unpadding encoders...\n",
      "Skipping long datasets for short encoders...\n",
      "\n",
      "Processing Time Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "fixed_short: 170.79 ± 0.06 seconds (batch_size: 128)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "n_iters = 10\n",
    "times = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        if 'short' not in dataset_name:\n",
    "            print(\"Skipping long datasets for short encoders...\")\n",
    "            continue\n",
    "        if 'fixed' not in dataset_name:\n",
    "            print(\"Skipping variable datasets for no-unpadding encoders...\")\n",
    "            continue\n",
    "        print(f\"\\nTesting {dataset_name}...\")\n",
    "        batch_times = []\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.TensorDataset(\n",
    "                dataset['input_ids'].to('cuda:0'),\n",
    "                dataset['attention_mask'].to('cuda:0')\n",
    "            ),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Warmup\n",
    "        for batch in dataloader:\n",
    "            model(input_ids=batch[0], attention_mask=batch[1])\n",
    "            break\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Timing runs\n",
    "        for i in range(n_iters):\n",
    "            start = time.perf_counter()\n",
    "            for batch in dataloader:\n",
    "                model(input_ids=batch[0], attention_mask=batch[1])\n",
    "            torch.cuda.synchronize()\n",
    "            end = time.perf_counter()\n",
    "            batch_times.append(end - start)\n",
    "            \n",
    "        mean_time = np.mean(batch_times)\n",
    "        std_time = np.std(batch_times)\n",
    "        print(f\"{dataset_name} -> {mean_time:.2f} ± {std_time:.2f} sec (batch_size: {batch_size})\")\n",
    "        times.append((dataset_name, mean_time, std_time))\n",
    "\n",
    "    print(\"\\nProcessing Time Summary:\")\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "    for name, mean, std in times:\n",
    "        print(f\"{name}: {mean:.2f} ± {std:.2f} seconds (batch_size: {batch_size})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('runtime.txt', 'w') as f:\n",
    "    for name, mean, std in times:\n",
    "        f.write(f\"{name}: {mean:.2f} ± {std:.2f} seconds (batch_size: {batch_size})\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
