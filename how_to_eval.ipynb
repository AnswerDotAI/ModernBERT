{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run evaluation fine-tuning\n",
    "\n",
    "This documents how to run evaluations for fine-tuning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Login to huggingface\n",
    "```bash\n",
    "huggingface-cli login\n",
    "Enter your token (input will not be visible): \n",
    "Add token as git credential? (Y/n) n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the appropriate pre-training checkpoint and training config\n",
    "\n",
    "Each pre-training checkpoint has a corresponding training config, one for base, large, long-context, and so forth. Below are recommended pre-training checkpoints and training configs for each:\n",
    "\n",
    "`bert24-base`\n",
    "```bash\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}/*\" --local-dir pretrained_checkpoints\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}\" --local-dir pretrained_checkpoints\n",
    "```\n",
    "\n",
    "`bert24-large`\n",
    "```bash\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}/*\" --local-dir pretrained_checkpoints\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}\" --local-dir pretrained_checkpoints\n",
    "```\n",
    "\n",
    "`bert24-long-context`\n",
    "```bash\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}/*\" --local-dir pretrained_checkpoints\n",
    "huggingface-cli download {org}/{repo} --include \"{checkpoint_folder}\" --local-dir pretrained_checkpoints\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "1. Delete the `.cache` folder in the `pretrained_checkpoints` if HF creates one.\n",
    "2. If there are multiple checkpoints, use the last one (usually starts with \"ep-1\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run evaluations\n",
    "\n",
    "#### Option 1: Run for all checkpoints using `run_evals_from_checkpoints.py`\n",
    "\n",
    "Take a look at the required and optional args:\n",
    "```bash\n",
    "python run_evals_from_checkpoints.py --help\n",
    "```\n",
    "\n",
    "To make things easier, create a yaml (e.g., `run_evals_args.yaml`) with the required argument values. For example:\n",
    "```yaml\n",
    "# Checkpoint & Config Paths\n",
    "checkpoints: checkpoints\n",
    "train_config: {training_config_fpath}\n",
    "\n",
    "# W&B\n",
    "wandb_entity: \n",
    "track_run: true\n",
    "track_run_project: \n",
    "```\n",
    "To run the script, use:\n",
    "```bash\n",
    "python run_evals_from_checkpoints.py --config run_evals_args.yaml --quiet\n",
    "```\n",
    "\n",
    "#### Option 2: Run for a specific checkpoints using `ablation_eval.py`\n",
    "To run the script, use:\n",
    "```bash\n",
    "python abalation_eval.py {task_eval_config_fpath}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Tricks\n",
    "\n",
    "1. If you want to build a fine-tuning evaluation configuration yaml for a single task, you can follow the instructions above for running evals against all checkpoints using `run_evals_from_checkpoints.py` to create the necessary fine-tuing/eval yaml to use with `ablation_eval.py`.  Once its created, you can stop the script from running.\n",
    "\n",
    "2. `pip install nvitop` and run `nvitop` to monitor GPU usage for a much more useful and usable interface than `nvidia-smi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
